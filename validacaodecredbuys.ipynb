{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2nPIB4NixCk",
        "outputId": "97a777cd-3204-4436-8afd-3281e278276c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "                              AVALIAÇÃO COMPLETA DO MODELO\n",
            "====================================================================================================\n",
            "\n",
            "Dataset carregado: 1759 exemplos, 10 features\n",
            "Target: Class\n",
            "\n",
            "====================================================================================================\n",
            "ANÁLISE DE DESBALANCEAMENTO\n",
            "====================================================================================================\n",
            "\n",
            "Distribuição Original das Classes:\n",
            "\n",
            "┌─────────────┬────────────┬─────────────┐\n",
            "│   Classe    │  Exemplos  │ Proporção % │\n",
            "├─────────────┼────────────┼─────────────┤\n",
            "│   Classe 0  │    1267    │    72.03%   │\n",
            "│   Classe 1  │     492    │    27.97%   │\n",
            "└─────────────┴────────────┴─────────────┘\n",
            "\n",
            "Razão de Desbalanceamento: 2.58:1\n",
            "Nível de Desbalanceamento: SEVERO\n",
            "\n",
            " IMPACTO ESPERADO DO DESBALANCEAMENTO:\n",
            "   • Modelo pode ter viés para a classe majoritária\n",
            "   • Métricas como Acurácia podem ser enganosas\n",
            "   • Classe minoritária pode ter baixo Recall\n",
            "   • Técnicas de balanceamento são NECESSÁRIAS\n",
            "\n",
            "====================================================================================================\n",
            "COMPARAÇÃO: SEM vs COM BALANCEAMENTO\n",
            "====================================================================================================\n",
            "\n",
            "Testando SEM balanceamento (Baseline)...\n",
            "   ✓ Baseline - Macro F1: 0.9372, Acurácia: 0.9517\n",
            "\n",
            "Testando COM SMOTE...\n",
            "   ✓ SMOTE - Macro F1: 0.9308, Acurácia: 0.9460\n",
            "\n",
            "Testando COM ADASYN...\n",
            "   ✓ ADASYN - Macro F1: 0.9146, Acurácia: 0.9318\n",
            "\n",
            "Testando COM SMOTE + Tomek Links...\n",
            "   ✓ SMOTE+Tomek - Macro F1: 0.9415, Acurácia: 0.9545\n",
            "\n",
            "Testando COM Under-Sampling...\n",
            "   ✓ Under-Sampling - Macro F1: 0.9250, Acurácia: 0.9403\n",
            "\n",
            "====================================================================================================\n",
            "TABELA COMPARATIVA DE TÉCNICAS\n",
            "====================================================================================================\n",
            "\n",
            "          Técnica  Acurácia  Macro F1-Score  Weighted F1-Score  Macro Precision  Macro Recall\n",
            "SEM Balanceamento    0.9517          0.9372             0.9506           0.9601        0.9195\n",
            "            SMOTE    0.9460          0.9308             0.9452           0.9453        0.9187\n",
            "           ADASYN    0.9318          0.9146             0.9316           0.9173        0.9120\n",
            "      SMOTE+Tomek    0.9545          0.9415             0.9538           0.9583        0.9278\n",
            "   Under-Sampling    0.9403          0.9250             0.9401           0.9293        0.9211\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "MELHOR TÉCNICA: SMOTE+Tomek\n",
            "   Macro F1-Score: 0.9415\n",
            "   Melhoria sobre Baseline: +0.46%\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            " Gerando visualizações...\n",
            "   ✓ Gráficos salvos em: comparacao_tecnicas.png\n",
            "\n",
            "📊 Gerando matrizes de confusão...\n",
            "   ✓ Matrizes de confusão salvas em: matrizes_confusao.png\n",
            "\n",
            "====================================================================================================\n",
            "ANÁLISE DETALHADA POR CLASSE\n",
            "====================================================================================================\n",
            "\n",
            "Comparação: BASELINE vs SMOTE+Tomek\n",
            "\n",
            "┌─────────┬──────────────────────────────┬──────────────────────────────┐\n",
            "│ Classe  │      SEM Balanceamento       │           SMOTE+Tomek            │\n",
            "├─────────┼──────────────────────────────┼──────────────────────────────┤\n",
            "│         │  Prec   Recall   F1-Score    │  Prec   Recall   F1-Score    │\n",
            "├─────────┼──────────────────────────────┼──────────────────────────────┤\n",
            "│ Classe 0 │  0.944   0.992    0.967     │  0.951   0.988    0.969     │\n",
            "│ Classe 1 │  0.976   0.847    0.907     │  0.966   0.867    0.914     │\n",
            "└─────────┴──────────────────────────────┴──────────────────────────────┘\n",
            "\n",
            "MELHORIAS POR CLASSE:\n",
            "\n",
            "   Classe 0: ^ +0.18% (de 0.9674 para 0.9691)\n",
            "   Classe 1: ^ +0.76% (de 0.9071 para 0.9140)\n",
            "\n",
            "Gerando relatório final...\n",
            "   ✓ Relatório salvo em: RELATORIO_AVALIACAO.txt\n",
            "\n",
            "====================================================================================================\n",
            "AVALIAÇÃO COMPLETA CONCLUÍDA COM SUCESSO!\n",
            "====================================================================================================\n",
            "\n",
            "Arquivos gerados:\n",
            "   • RELATORIO_AVALIACAO.txt - Relatório completo\n",
            "   • comparacao_tecnicas.png - Gráficos comparativos\n",
            "   • matrizes_confusao.png   - Matrizes de confusão\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    f1_score, accuracy_score, precision_score, recall_score,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class AvaliacaoCompleta:\n",
        "    \"\"\"Classe para avaliação completa do modelo com análise de desbalanceamento\"\"\"\n",
        "\n",
        "    def __init__(self, filepath, target_column=None, features_selecionadas=None):\n",
        "        \"\"\"\n",
        "        Inicializa a avaliação\n",
        "\n",
        "        Args:\n",
        "            filepath: Caminho para o arquivo de dados\n",
        "            target_column: Nome da coluna target\n",
        "            features_selecionadas: Lista de features selecionadas\n",
        "        \"\"\"\n",
        "        print(\"=\" * 100)\n",
        "        print(\" \" * 30 + \"AVALIAÇÃO COMPLETA DO MODELO\")\n",
        "        print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "        # Carregar dados\n",
        "        if filepath.endswith('.csv'):\n",
        "            self.df = pd.read_csv(filepath)\n",
        "        else:\n",
        "            self.df = pd.read_excel(filepath)\n",
        "\n",
        "        # Definir target\n",
        "        if target_column is None:\n",
        "            target_column = self.df.columns[-1]\n",
        "\n",
        "        self.target_column = target_column\n",
        "\n",
        "        # Selecionar features\n",
        "        if features_selecionadas is None:\n",
        "            self.X = self.df.drop(columns=[target_column])\n",
        "        else:\n",
        "            self.X = self.df[features_selecionadas]\n",
        "\n",
        "        self.y = self.df[target_column]\n",
        "\n",
        "        print(f\"Dataset carregado: {self.df.shape[0]} exemplos, {self.X.shape[1]} features\")\n",
        "        print(f\"Target: {target_column}\")\n",
        "\n",
        "        # Analisar desbalanceamento\n",
        "        self._analisar_desbalanceamento()\n",
        "\n",
        "    def _analisar_desbalanceamento(self):\n",
        "        \"\"\"Analisa o desbalanceamento das classes\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"ANÁLISE DE DESBALANCEAMENTO\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        contagem = self.y.value_counts().sort_index()\n",
        "        proporcoes = self.y.value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "        print(\"\\nDistribuição Original das Classes:\\n\")\n",
        "        print(\"┌─────────────┬────────────┬─────────────┐\")\n",
        "        print(\"│   Classe    │  Exemplos  │ Proporção % │\")\n",
        "        print(\"├─────────────┼────────────┼─────────────┤\")\n",
        "        for classe in contagem.index:\n",
        "            count = contagem[classe]\n",
        "            prop = proporcoes[classe]\n",
        "            print(f\"│   Classe {classe}  │   {count:5d}    │    {prop:5.2f}%   │\")\n",
        "        print(\"└─────────────┴────────────┴─────────────┘\")\n",
        "\n",
        "        # Calcular razão\n",
        "        razao = contagem.max() / contagem.min()\n",
        "        self.razao_desbalanceamento = razao\n",
        "\n",
        "        print(f\"\\nRazão de Desbalanceamento: {razao:.2f}:1\")\n",
        "\n",
        "        if razao > 2:\n",
        "            nivel = \"SEVERO\"\n",
        "        elif razao > 1.5:\n",
        "            nivel = \"MODERADO\"\n",
        "        else:\n",
        "            nivel = \"BAIXO\"\n",
        "\n",
        "\n",
        "        print(f\"Nível de Desbalanceamento: {nivel}\")\n",
        "\n",
        "        if razao > 1.5:\n",
        "            print(\"\\n IMPACTO ESPERADO DO DESBALANCEAMENTO:\")\n",
        "            print(\"   • Modelo pode ter viés para a classe majoritária\")\n",
        "            print(\"   • Métricas como Acurácia podem ser enganosas\")\n",
        "            print(\"   • Classe minoritária pode ter baixo Recall\")\n",
        "            print(\"   • Técnicas de balanceamento são NECESSÁRIAS\")\n",
        "\n",
        "    def comparar_tecnicas_balanceamento(self, modelo_nome='Random Forest',\n",
        "                                       features_selecionadas=None, cv_folds=5):\n",
        "        \"\"\"\n",
        "        Compara o desempenho do modelo SEM e COM diferentes técnicas de balanceamento\n",
        "\n",
        "        Args:\n",
        "            modelo_nome: Nome do modelo a usar\n",
        "            features_selecionadas: Lista de features\n",
        "            cv_folds: Número de folds\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"COMPARAÇÃO: SEM vs COM BALANCEAMENTO\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        # Preparar dados\n",
        "        if features_selecionadas is None:\n",
        "            X = self.X\n",
        "        else:\n",
        "            X = self.X[features_selecionadas]\n",
        "\n",
        "        # Split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
        "        )\n",
        "\n",
        "        # Padronizar\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Selecionar modelo\n",
        "        if modelo_nome == 'Random Forest':\n",
        "            modelo_base = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "        elif modelo_nome == 'Gradient Boosting':\n",
        "            modelo_base = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
        "        else:\n",
        "            modelo_base = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "        resultados = {}\n",
        "\n",
        "        # 1. SEM BALANCEAMENTO (BASELINE)\n",
        "        print(\"\\nTestando SEM balanceamento (Baseline)...\")\n",
        "        modelo = modelo_base.__class__(**modelo_base.get_params())\n",
        "        modelo.fit(X_train_scaled, y_train)\n",
        "        y_pred = modelo.predict(X_test_scaled)\n",
        "\n",
        "        resultados['SEM Balanceamento'] = self._calcular_metricas(y_test, y_pred, \"Baseline\")\n",
        "\n",
        "        # 2. COM SMOTE\n",
        "        print(\"\\nTestando COM SMOTE...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_smote, y_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        modelo = modelo_base.__class__(**modelo_base.get_params())\n",
        "        modelo.fit(X_smote, y_smote)\n",
        "        y_pred = modelo.predict(X_test_scaled)\n",
        "\n",
        "        resultados['SMOTE'] = self._calcular_metricas(y_test, y_pred, \"SMOTE\")\n",
        "\n",
        "        # 3. COM ADASYN\n",
        "        print(\"\\nTestando COM ADASYN...\")\n",
        "        try:\n",
        "            adasyn = ADASYN(random_state=42)\n",
        "            X_adasyn, y_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "            modelo = modelo_base.__class__(**modelo_base.get_params())\n",
        "            modelo.fit(X_adasyn, y_adasyn)\n",
        "            y_pred = modelo.predict(X_test_scaled)\n",
        "\n",
        "            resultados['ADASYN'] = self._calcular_metricas(y_test, y_pred, \"ADASYN\")\n",
        "        except:\n",
        "            print(\"ADASYN falhou (possível falta de vizinhos)\")\n",
        "            resultados['ADASYN'] = None\n",
        "\n",
        "        # 4. COM SMOTE + Tomek\n",
        "        print(\"\\nTestando COM SMOTE + Tomek Links...\")\n",
        "        smotetomek = SMOTETomek(random_state=42)\n",
        "        X_st, y_st = smotetomek.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        modelo = modelo_base.__class__(**modelo_base.get_params())\n",
        "        modelo.fit(X_st, y_st)\n",
        "        y_pred = modelo.predict(X_test_scaled)\n",
        "\n",
        "        resultados['SMOTE+Tomek'] = self._calcular_metricas(y_test, y_pred, \"SMOTE+Tomek\")\n",
        "\n",
        "        # 5. COM Under-sampling\n",
        "        print(\"\\nTestando COM Under-Sampling...\")\n",
        "        under = RandomUnderSampler(random_state=42)\n",
        "        X_under, y_under = under.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        modelo = modelo_base.__class__(**modelo_base.get_params())\n",
        "        modelo.fit(X_under, y_under)\n",
        "        y_pred = modelo.predict(X_test_scaled)\n",
        "\n",
        "        resultados['Under-Sampling'] = self._calcular_metricas(y_test, y_pred, \"Under-Sampling\")\n",
        "\n",
        "        self.resultados_comparacao = resultados\n",
        "        self.X_test = X_test_scaled\n",
        "        self.y_test = y_test\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def _calcular_metricas(self, y_true, y_pred, nome_tecnica):\n",
        "        \"\"\"Calcula todas as métricas necessárias\"\"\"\n",
        "\n",
        "        metricas = {\n",
        "            'Acurácia': accuracy_score(y_true, y_pred),\n",
        "            'Macro F1-Score': f1_score(y_true, y_pred, average='macro'),\n",
        "            'Weighted F1-Score': f1_score(y_true, y_pred, average='weighted'),\n",
        "            'Macro Precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "            'Macro Recall': recall_score(y_true, y_pred, average='macro'),\n",
        "            'Confusion Matrix': confusion_matrix(y_true, y_pred)\n",
        "        }\n",
        "\n",
        "        # Métricas por classe\n",
        "        report = classification_report(y_true, y_pred, output_dict=True)\n",
        "        metricas['Report'] = report\n",
        "\n",
        "        print(f\"   ✓ {nome_tecnica} - Macro F1: {metricas['Macro F1-Score']:.4f}, Acurácia: {metricas['Acurácia']:.4f}\")\n",
        "\n",
        "        return metricas\n",
        "\n",
        "    def gerar_tabela_comparativa(self):\n",
        "        \"\"\"Gera tabela comparativa das técnicas\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"TABELA COMPARATIVA DE TÉCNICAS\")\n",
        "        print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "        # Criar DataFrame\n",
        "        data = []\n",
        "        for tecnica, metricas in self.resultados_comparacao.items():\n",
        "            if metricas is not None:\n",
        "                data.append({\n",
        "                    'Técnica': tecnica,\n",
        "                    'Acurácia': metricas['Acurácia'],\n",
        "                    'Macro F1-Score': metricas['Macro F1-Score'],\n",
        "                    'Weighted F1-Score': metricas['Weighted F1-Score'],\n",
        "                    'Macro Precision': metricas['Macro Precision'],\n",
        "                    'Macro Recall': metricas['Macro Recall']\n",
        "                })\n",
        "\n",
        "        df_comp = pd.DataFrame(data)\n",
        "        df_comp = df_comp.round(4)\n",
        "\n",
        "        print(df_comp.to_string(index=False))\n",
        "\n",
        "        # Identificar melhor técnica\n",
        "        melhor_idx = df_comp['Macro F1-Score'].idxmax()\n",
        "        melhor_tecnica = df_comp.iloc[melhor_idx]['Técnica']\n",
        "        melhor_f1 = df_comp.iloc[melhor_idx]['Macro F1-Score']\n",
        "\n",
        "        baseline_f1 = df_comp[df_comp['Técnica'] == 'SEM Balanceamento']['Macro F1-Score'].values[0]\n",
        "        melhoria = ((melhor_f1 - baseline_f1) / baseline_f1) * 100\n",
        "\n",
        "        print(\"\\n\" + \"─\" * 100)\n",
        "        print(f\"MELHOR TÉCNICA: {melhor_tecnica}\")\n",
        "        print(f\"   Macro F1-Score: {melhor_f1:.4f}\")\n",
        "        print(f\"   Melhoria sobre Baseline: +{melhoria:.2f}%\")\n",
        "        print(\"─\" * 100)\n",
        "\n",
        "        self.df_comparacao = df_comp\n",
        "        self.melhor_tecnica = melhor_tecnica\n",
        "\n",
        "        return df_comp\n",
        "\n",
        "    def visualizar_comparacao(self, output_file='comparacao_tecnicas.png'):\n",
        "        \"\"\"Cria visualizações comparativas\"\"\"\n",
        "        print(f\"\\n Gerando visualizações...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle('Comparação de Técnicas de Balanceamento',\n",
        "                     fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "        # 1. Acurácia\n",
        "        ax = axes[0, 0]\n",
        "        self.df_comparacao.plot(x='Técnica', y='Acurácia', kind='bar',\n",
        "                                ax=ax, color='steelblue', legend=False)\n",
        "        ax.set_title('Acurácia', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Destacar melhor\n",
        "        melhor_idx = self.df_comparacao['Acurácia'].idxmax()\n",
        "        ax.patches[melhor_idx].set_color('gold')\n",
        "        ax.patches[melhor_idx].set_edgecolor('black')\n",
        "        ax.patches[melhor_idx].set_linewidth(3)\n",
        "\n",
        "        # 2. Macro F1-Score\n",
        "        ax = axes[0, 1]\n",
        "        self.df_comparacao.plot(x='Técnica', y='Macro F1-Score', kind='bar',\n",
        "                                ax=ax, color='coral', legend=False)\n",
        "        ax.set_title('Macro F1-Score (Métrica Principal)', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Destacar melhor\n",
        "        melhor_idx = self.df_comparacao['Macro F1-Score'].idxmax()\n",
        "        ax.patches[melhor_idx].set_color('gold')\n",
        "        ax.patches[melhor_idx].set_edgecolor('black')\n",
        "        ax.patches[melhor_idx].set_linewidth(3)\n",
        "\n",
        "        # 3. Weighted F1-Score\n",
        "        ax = axes[0, 2]\n",
        "        self.df_comparacao.plot(x='Técnica', y='Weighted F1-Score', kind='bar',\n",
        "                                ax=ax, color='mediumseagreen', legend=False)\n",
        "        ax.set_title('Weighted F1-Score', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 4. Macro Precision\n",
        "        ax = axes[1, 0]\n",
        "        self.df_comparacao.plot(x='Técnica', y='Macro Precision', kind='bar',\n",
        "                                ax=ax, color='plum', legend=False)\n",
        "        ax.set_title('Macro Precision', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 5. Macro Recall\n",
        "        ax = axes[1, 1]\n",
        "        self.df_comparacao.plot(x='Técnica', y='Macro Recall', kind='bar',\n",
        "                                ax=ax, color='lightsalmon', legend=False)\n",
        "        ax.set_title('Macro Recall', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 6. Comparação lado a lado\n",
        "        ax = axes[1, 2]\n",
        "        metricas_principais = self.df_comparacao[['Técnica', 'Acurácia', 'Macro F1-Score',\n",
        "                                                   'Macro Precision', 'Macro Recall']].set_index('Técnica')\n",
        "        metricas_principais.plot(kind='bar', ax=ax, width=0.8)\n",
        "        ax.set_title('Todas as Métricas', fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel('Score', fontsize=12)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylim([0.85, 1.0])\n",
        "        ax.legend(loc='lower right', fontsize=9)\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   ✓ Gráficos salvos em: {output_file}\")\n",
        "        plt.close()\n",
        "\n",
        "    def visualizar_matrizes_confusao(self, output_file='matrizes_confusao.png'):\n",
        "        \"\"\"Cria visualização das matrizes de confusão\"\"\"\n",
        "        print(f\"\\nGerando matrizes de confusão...\")\n",
        "\n",
        "        # Filtrar técnicas válidas\n",
        "        tecnicas_validas = {k: v for k, v in self.resultados_comparacao.items() if v is not None}\n",
        "        n_tecnicas = len(tecnicas_validas)\n",
        "\n",
        "        # Criar subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Matrizes de Confusão - Comparação de Técnicas',\n",
        "                     fontsize=16, fontweight='bold')\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for idx, (tecnica, metricas) in enumerate(tecnicas_validas.items()):\n",
        "            cm = metricas['Confusion Matrix']\n",
        "\n",
        "            # Calcular percentuais\n",
        "            cm_percentual = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "            # Plotar\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                       cbar=False, square=True, linewidths=2, linecolor='white',\n",
        "                       annot_kws={'size': 14, 'weight': 'bold'})\n",
        "\n",
        "            # Adicionar percentuais\n",
        "            for i in range(cm.shape[0]):\n",
        "                for j in range(cm.shape[1]):\n",
        "                    axes[idx].text(j + 0.5, i + 0.7, f'({cm_percentual[i, j]:.1f}%)',\n",
        "                                 ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "            axes[idx].set_title(f'{tecnica}\\nF1-Macro: {metricas[\"Macro F1-Score\"]:.4f}',\n",
        "                              fontsize=12, fontweight='bold')\n",
        "            axes[idx].set_ylabel('Classe Real', fontsize=11)\n",
        "            axes[idx].set_xlabel('Classe Predita', fontsize=11)\n",
        "\n",
        "            # Destacar melhor técnica\n",
        "            if tecnica == self.melhor_tecnica:\n",
        "                for spine in axes[idx].spines.values():\n",
        "                    spine.set_edgecolor('gold')\n",
        "                    spine.set_linewidth(4)\n",
        "\n",
        "        # Remover subplots não utilizados\n",
        "        for idx in range(n_tecnicas, len(axes)):\n",
        "            fig.delaxes(axes[idx])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "        print(f\"   ✓ Matrizes de confusão salvas em: {output_file}\")\n",
        "        plt.close()\n",
        "\n",
        "    def analisar_metricas_por_classe(self):\n",
        "        \"\"\"Analisa métricas detalhadas por classe\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"ANÁLISE DETALHADA POR CLASSE\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        # Comparar baseline vs melhor técnica\n",
        "        baseline = self.resultados_comparacao['SEM Balanceamento']\n",
        "        melhor = self.resultados_comparacao[self.melhor_tecnica]\n",
        "\n",
        "        print(f\"\\nComparação: BASELINE vs {self.melhor_tecnica}\\n\")\n",
        "\n",
        "        # Criar tabela\n",
        "        classes = sorted(baseline['Report'].keys())\n",
        "        classes = [c for c in classes if c not in ['accuracy', 'macro avg', 'weighted avg']]\n",
        "\n",
        "        print(\"┌─────────┬──────────────────────────────┬──────────────────────────────┐\")\n",
        "        print(f\"│ Classe  │      SEM Balanceamento       │       {self.melhor_tecnica:^20s}       │\")\n",
        "        print(\"├─────────┼──────────────────────────────┼──────────────────────────────┤\")\n",
        "        print(\"│         │  Prec   Recall   F1-Score    │  Prec   Recall   F1-Score    │\")\n",
        "        print(\"├─────────┼──────────────────────────────┼──────────────────────────────┤\")\n",
        "\n",
        "        for classe in classes:\n",
        "            base_metrics = baseline['Report'][str(classe)]\n",
        "            melhor_metrics = melhor['Report'][str(classe)]\n",
        "\n",
        "            print(f\"│ Classe {classe} │  {base_metrics['precision']:.3f}   {base_metrics['recall']:.3f}    {base_metrics['f1-score']:.3f}     │\" +\n",
        "                  f\"  {melhor_metrics['precision']:.3f}   {melhor_metrics['recall']:.3f}    {melhor_metrics['f1-score']:.3f}     │\")\n",
        "\n",
        "        print(\"└─────────┴──────────────────────────────┴──────────────────────────────┘\")\n",
        "\n",
        "        # Análise de melhorias\n",
        "        print(\"\\nMELHORIAS POR CLASSE:\\n\")\n",
        "        for classe in classes:\n",
        "            base_f1 = baseline['Report'][str(classe)]['f1-score']\n",
        "            melhor_f1 = melhor['Report'][str(classe)]['f1-score']\n",
        "            melhoria = ((melhor_f1 - base_f1) / base_f1) * 100\n",
        "\n",
        "            seta = \"^\" if melhoria > 0 else \"\" if melhoria < 0 else \">\"\n",
        "            print(f\"   Classe {classe}: {seta} {melhoria:+.2f}% (de {base_f1:.4f} para {melhor_f1:.4f})\")\n",
        "\n",
        "    def gerar_relatorio_final(self, output_file='RELATORIO_AVALIACAO.txt'):\n",
        "        \"\"\"Gera relatório final completo\"\"\"\n",
        "        print(f\"\\nGerando relatório final...\")\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\" * 100 + \"\\n\")\n",
        "            f.write(\" \" * 25 + \"RELATÓRIO DE AVALIAÇÃO DO MODELO\\n\")\n",
        "            f.write(\" \" * 20 + \"Análise do Impacto do Desbalanceamento\\n\")\n",
        "            f.write(\"=\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            # 1. INFORMAÇÕES DO DATASET\n",
        "            f.write(\"1. INFORMAÇÕES DO DATASET\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "            f.write(f\"Total de Exemplos: {len(self.df)}\\n\")\n",
        "            f.write(f\"Número de Features: {self.X.shape[1]}\\n\")\n",
        "            f.write(f\"Target: {self.target_column}\\n\\n\")\n",
        "\n",
        "            # Distribuição\n",
        "            contagem = self.y.value_counts().sort_index()\n",
        "            proporcoes = self.y.value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "            f.write(\"Distribuição de Classes:\\n\")\n",
        "            for classe in contagem.index:\n",
        "                f.write(f\"  Classe {classe}: {contagem[classe]:5d} exemplos ({proporcoes[classe]:5.2f}%)\\n\")\n",
        "\n",
        "            f.write(f\"\\nRazão de Desbalanceamento: {self.razao_desbalanceamento:.2f}:1\\n\")\n",
        "\n",
        "            # 2. IMPACTO DO DESBALANCEAMENTO\n",
        "            f.write(\"\\n\\n2. IMPACTO DO DESBALANCEAMENTO\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            baseline = self.resultados_comparacao['SEM Balanceamento']\n",
        "\n",
        "            f.write(\"MODELO SEM BALANCEAMENTO (Baseline):\\n\\n\")\n",
        "            f.write(f\"  • Acurácia:        {baseline['Acurácia']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro F1-Score:  {baseline['Macro F1-Score']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro Precision: {baseline['Macro Precision']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro Recall:    {baseline['Macro Recall']:.4f}\\n\\n\")\n",
        "\n",
        "            f.write(\"PROBLEMAS IDENTIFICADOS:\\n\\n\")\n",
        "\n",
        "            # Analisar problemas\n",
        "            report = baseline['Report']\n",
        "            classes = [c for c in report.keys() if c not in ['accuracy', 'macro avg', 'weighted avg']]\n",
        "\n",
        "            # Identificar classe minoritária\n",
        "            contagem = self.y.value_counts()\n",
        "            classe_minoritaria = str(contagem.idxmin())\n",
        "            classe_majoritaria = str(contagem.idxmax())\n",
        "\n",
        "            recall_min = report[classe_minoritaria]['recall']\n",
        "            recall_maj = report[classe_majoritaria]['recall']\n",
        "\n",
        "            if recall_min < recall_maj - 0.05:\n",
        "                f.write(f\"   Classe minoritária ({classe_minoritaria}) com Recall significativamente menor\\n\")\n",
        "                f.write(f\"      Recall Classe {classe_minoritaria}: {recall_min:.4f}\\n\")\n",
        "                f.write(f\"      Recall Classe {classe_majoritaria}: {recall_maj:.4f}\\n\")\n",
        "                f.write(f\"      Diferença: {(recall_maj - recall_min):.4f}\\n\\n\")\n",
        "\n",
        "            f.write(\"  Modelo pode estar enviesado para a classe majoritária\\n\")\n",
        "            f.write(\"  Acurácia alta pode ser enganosa devido ao desbalanceamento\\n\")\n",
        "            f.write(\"  Macro F1-Score é mais confiável que Acurácia neste caso\\n\")\n",
        "\n",
        "            # 3. TÉCNICAS APLICADAS\n",
        "            f.write(\"\\n\\n3. TÉCNICAS DE BALANCEAMENTO APLICADAS\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"Foram testadas 5 abordagens:\\n\\n\")\n",
        "\n",
        "            f.write(\"1. SEM Balanceamento (Baseline)\\n\")\n",
        "            f.write(\"   • Modelo treinado com dados originais desbalanceados\\n\")\n",
        "            f.write(\"   • Serve como referência para medir melhorias\\n\\n\")\n",
        "\n",
        "            f.write(\"2. SMOTE (Synthetic Minority Over-sampling Technique)\\n\")\n",
        "            f.write(\"   • Cria exemplos sintéticos da classe minoritária\\n\")\n",
        "            f.write(\"   • Gera novos pontos interpolando exemplos existentes\\n\")\n",
        "            f.write(\"   • Aumenta o tamanho do conjunto de treino\\n\\n\")\n",
        "\n",
        "            f.write(\"3. ADASYN (Adaptive Synthetic Sampling)\\n\")\n",
        "            f.write(\"   • Similar ao SMOTE, mas adaptativo\\n\")\n",
        "            f.write(\"   • Foca em gerar exemplos em regiões difíceis de classificar\\n\")\n",
        "            f.write(\"   • Mais sensível a outliers\\n\\n\")\n",
        "\n",
        "            f.write(\"4. SMOTE + Tomek Links\\n\")\n",
        "            f.write(\"   • Combina SMOTE com limpeza de fronteira\\n\")\n",
        "            f.write(\"   • Remove exemplos ambíguos após sobre-amostragem\\n\")\n",
        "            f.write(\"   • Melhora a definição da fronteira de decisão\\n\\n\")\n",
        "\n",
        "            f.write(\"5. Under-Sampling Aleatório\\n\")\n",
        "            f.write(\"   • Remove aleatoriamente exemplos da classe majoritária\\n\")\n",
        "            f.write(\"   • Reduz o tamanho do conjunto de treino\\n\")\n",
        "            f.write(\"   • Pode perder informação importante\\n\\n\")\n",
        "\n",
        "            # 4. RESULTADOS COMPARATIVOS\n",
        "            f.write(\"\\n4. RESULTADOS COMPARATIVOS\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            f.write(self.df_comparacao.to_string(index=False))\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "            # 5. MELHOR TÉCNICA\n",
        "            f.write(\"\\n5. MELHOR TÉCNICA IDENTIFICADA\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            melhor = self.resultados_comparacao[self.melhor_tecnica]\n",
        "            baseline_f1 = baseline['Macro F1-Score']\n",
        "            melhor_f1 = melhor['Macro F1-Score']\n",
        "            melhoria = ((melhor_f1 - baseline_f1) / baseline_f1) * 100\n",
        "\n",
        "            f.write(f\"TÉCNICA VENCEDORA: {self.melhor_tecnica}\\n\\n\")\n",
        "            f.write(f\"MÉTRICAS FINAIS:\\n\\n\")\n",
        "            f.write(f\"  • Acurácia:        {melhor['Acurácia']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro F1-Score:  {melhor['Macro F1-Score']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro Precision: {melhor['Macro Precision']:.4f}\\n\")\n",
        "            f.write(f\"  • Macro Recall:    {melhor['Macro Recall']:.4f}\\n\\n\")\n",
        "\n",
        "            f.write(f\"MELHORIA SOBRE BASELINE:\\n\\n\")\n",
        "            f.write(f\"  • Macro F1-Score:  +{melhoria:.2f}%\\n\")\n",
        "            f.write(f\"  • De {baseline_f1:.4f} para {melhor_f1:.4f}\\n\\n\")\n",
        "\n",
        "            # 6. MATRIZ DE CONFUSÃO\n",
        "            f.write(\"\\n6. MATRIZ DE CONFUSÃO - MELHOR TÉCNICA\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            cm = melhor['Confusion Matrix']\n",
        "            f.write(\"Matriz de Confusão:\\n\\n\")\n",
        "            f.write(str(cm))\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "            # Análise da matriz\n",
        "            f.write(\"INTERPRETAÇÃO:\\n\\n\")\n",
        "\n",
        "            # Para classificação binária\n",
        "            if cm.shape[0] == 2:\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "                f.write(f\"  Verdadeiros Negativos (TN): {tn}\\n\")\n",
        "                f.write(f\"  Falsos Positivos (FP):      {fp}\\n\")\n",
        "                f.write(f\"  Falsos Negativos (FN):      {fn}\\n\")\n",
        "                f.write(f\"  Verdadeiros Positivos (TP): {tp}\\n\\n\")\n",
        "\n",
        "                # Calcular taxas\n",
        "                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "                f.write(f\"  Taxa de Verdadeiros Positivos (Recall Classe 1): {tpr:.4f}\\n\")\n",
        "                f.write(f\"  Taxa de Verdadeiros Negativos (Recall Classe 0): {tnr:.4f}\\n\\n\")\n",
        "\n",
        "                if abs(tpr - tnr) < 0.05:\n",
        "                    f.write(\"  Modelo balanceado entre as classes\\n\")\n",
        "                else:\n",
        "                    f.write(\" Modelo ainda apresenta algum viés\\n\")\n",
        "\n",
        "            # 7. ANÁLISE POR CLASSE\n",
        "            f.write(\"\\n\\n7. DESEMPENHO POR CLASSE\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"COMPARAÇÃO: Baseline vs Melhor Técnica\\n\\n\")\n",
        "\n",
        "            classes = [c for c in melhor['Report'].keys() if c not in ['accuracy', 'macro avg', 'weighted avg']]\n",
        "\n",
        "            for classe in classes:\n",
        "                base_metrics = baseline['Report'][str(classe)]\n",
        "                melhor_metrics = melhor['Report'][str(classe)]\n",
        "\n",
        "                f.write(f\"Classe {classe}:\\n\")\n",
        "                f.write(f\"  Baseline:      Precision={base_metrics['precision']:.4f}, \" +\n",
        "                       f\"Recall={base_metrics['recall']:.4f}, F1={base_metrics['f1-score']:.4f}\\n\")\n",
        "                f.write(f\"  {self.melhor_tecnica}: Precision={melhor_metrics['precision']:.4f}, \" +\n",
        "                       f\"Recall={melhor_metrics['recall']:.4f}, F1={melhor_metrics['f1-score']:.4f}\\n\")\n",
        "\n",
        "                melhoria_f1 = ((melhor_metrics['f1-score'] - base_metrics['f1-score']) /\n",
        "                              base_metrics['f1-score']) * 100\n",
        "                f.write(f\"  Melhoria F1:   {melhoria_f1:+.2f}%\\n\\n\")\n",
        "\n",
        "            # 8. CONCLUSÕES\n",
        "            f.write(\"\\n8. CONCLUSÕES E RECOMENDAÇÕES\\n\")\n",
        "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"PRINCIPAIS DESCOBERTAS:\\n\\n\")\n",
        "\n",
        "            f.write(f\"1. O dataset apresenta desbalanceamento {self.razao_desbalanceamento:.2f}:1\\n\\n\")\n",
        "\n",
        "            f.write(f\"2. O uso de {self.melhor_tecnica} melhorou significativamente o desempenho:\\n\")\n",
        "            f.write(f\"   • Macro F1-Score aumentou em {melhoria:.2f}%\\n\")\n",
        "            f.write(f\"   • Modelo ficou mais balanceado entre as classes\\n\\n\")\n",
        "\n",
        "            f.write(\"3. Métricas adequadas para dados desbalanceados:\\n\")\n",
        "            f.write(\"   • Macro F1-Score é a métrica principal (equilibra precisão e recall)\\n\")\n",
        "            f.write(\"   • Matriz de Confusão revela padrões de erro por classe\\n\")\n",
        "            f.write(\"   • Acurácia sozinha pode ser enganosa\\n\\n\")\n",
        "\n",
        "            f.write(\"RECOMENDAÇÕES:\\n\\n\")\n",
        "            f.write(f\"• Utilizar {self.melhor_tecnica} para treinar o modelo final\\n\")\n",
        "            f.write(\"• Monitorar Macro F1-Score como métrica principal\\n\")\n",
        "            f.write(\"• Analisar a Matriz de Confusão regularmente\\n\")\n",
        "            f.write(\"• Considerar ajuste de hiperparâmetros para melhorias adicionais\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
        "            f.write(\"RELATÓRIO GERADO COM SUCESSO\\n\")\n",
        "            f.write(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "        print(f\"   ✓ Relatório salvo em: {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    FILEPATH = 'datasample.csv'\n",
        "    TARGET_COLUMN = 'Class'\n",
        "\n",
        "    FEATURES_SELECIONADAS = [\n",
        "        'V14', 'V12', 'V4', 'V11', 'V10',\n",
        "        'V16', 'V17', 'V3', 'V9', 'V7'\n",
        "    ]\n",
        "\n",
        "    MODELO = 'Random Forest'\n",
        "\n",
        "    try:\n",
        "        # 1. Inicializar avaliação\n",
        "        avaliacao = AvaliacaoCompleta(\n",
        "            filepath=FILEPATH,\n",
        "            target_column=TARGET_COLUMN,\n",
        "            features_selecionadas=FEATURES_SELECIONADAS\n",
        "        )\n",
        "\n",
        "        # 2. Comparar técnicas de balanceamento\n",
        "        resultados = avaliacao.comparar_tecnicas_balanceamento(\n",
        "            modelo_nome=MODELO,\n",
        "            features_selecionadas=FEATURES_SELECIONADAS\n",
        "        )\n",
        "\n",
        "        # 3. Gerar tabela comparativa\n",
        "        df_comparacao = avaliacao.gerar_tabela_comparativa()\n",
        "\n",
        "        # 4. Visualizações\n",
        "        avaliacao.visualizar_comparacao('comparacao_tecnicas.png')\n",
        "        avaliacao.visualizar_matrizes_confusao('matrizes_confusao.png')\n",
        "\n",
        "        # 5. Análise por classe\n",
        "        avaliacao.analisar_metricas_por_classe()\n",
        "\n",
        "        # 6. Relatório final\n",
        "        avaliacao.gerar_relatorio_final('RELATORIO_AVALIACAO.txt')\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"AVALIAÇÃO COMPLETA CONCLUÍDA COM SUCESSO!\")\n",
        "        print(\"=\" * 100)\n",
        "        print(\"\\nArquivos gerados:\")\n",
        "        print(\"   • RELATORIO_AVALIACAO.txt - Relatório completo\")\n",
        "        print(\"   • comparacao_tecnicas.png - Gráficos comparativos\")\n",
        "        print(\"   • matrizes_confusao.png   - Matrizes de confusão\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERRO: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    }
  ]
}
